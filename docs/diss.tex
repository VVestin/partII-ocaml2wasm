% Template for a Computer Science Tripos Part II project dissertation
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage{hyperref}
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{mwe}
\usepackage{float}

\graphicspath{ {images/} }

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\rightline{\LARGE \textbf{Weston Metzler}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{Compiling OCaml to WebAssembly} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Homerton College \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Weston Metzler                       \\
College:            & \bf Homerton College                      \\
Project Title:      & \bf Compiling OCaml to WebAssembly         \\
Examination:        & \bf Computer Science Tripos -- Part II, July 2021  \\
Word Count:         & \bf TODO \\
Code Line Count:    & \bf TODO \\
Project Originator: & Dr Timothy Jones                    \\
Supervisor:         & Dr John Fawcett                    \\
\end{tabular}
}
\stepcounter{footnote}

\section*{Original Aims of the Project}

TODO

\section*{Work Completed}

TODO

\section*{Special Difficulties}

TODO

\newpage
\section*{Declaration}

I, Weston Metzler of Homerton College, being a candidate for Part II of the Computer
Science Tripos, hereby declare that this dissertation and the work described in
it are my own work, unaided except as may be specified below, and that the
dissertation does not contain material that has already been used to any substantial
extent for a comparable purpose.

\bigskip
\leftline{Signed [signature]}

\medskip
\leftline{Date [date]}

\tableofcontents

\listoffigures

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\pagestyle{headings}

\chapter{Introduction}

TODO

\chapter{Preparation}

This chapter describes the work done prior to starting the project.
We start with background information on WebAssembly and the tools and techniques used in the implementation.
Then we will look into the requirements analysis for this project, laying out in detail the work to be done.
Finally, we will look at the software engineering practices used to minimize risk and ensure the execution of the project meets the requirements.

\section{Background}
\subsection{WebAssembly}

In the introduction, we talked about the role of WebAssembly as an alternative execution environment for web browsers to JavaScript.
Now, we're going to dive a bit deeper into how WebAssembly works for reference in later chapters.
At a high level, WebAssembly is a general purpose specification and binary instruction format.
The instruction set is quite low level, resembling assembly languages to take advantage of hardware and run at near native speeds.
That said, it's not specific to any architecture, and is designed to run on a stack-based virtual machine, giving it portability along with speed and efficiency.
Currently the main target environment is web browsers as the name WebAssembly suggests, but WebAssembly can easily be run on a range of platforms.
Besides offering near-native performance and better hardware access to web applications, a main aim of WebAssembly is safety.
WebAssembly runs in a sandboxed environment with security guarantees, which is hugely important for web applications where code is downloaded from untrusted sources.

Key features of WebAssembly:
\begin{itemize}
   \item \textbf{Stack Machine}
      WebAssembly operates on a stack-based VM.
      This simplifies our implementation because we don't have to manually manage the stack.
      It also leads to really natural code generation straight from the AST without needing an 3 Address Code (3AC) like intermediate representation.
      This also means we don't have to worry about register allocation, which is an NP-hard problem to do so optimally.
   \item \textbf{Types}
      WebAssembly, like any assembly language only supports primitive integers and floats.
      Both 32 and 64 bit variants are available, but for simplicity our OCaml only has {\tt int}  and {\tt float} types, so we'll focus on the 32 bit variants.
      For the two supported compound types (tuples and datatypes), we'll have to design heap-allocated representations for our compiler.
   \item \textbf{Local \& Global Variables}
      The WebAssembly text format was designed with readability in mind, and supports named local and global variables as well as functions.
   \item \textbf{Memory Management}
      WebAssembly does not have any support for garbage collection.
      You can describe memory blocks with the {\tt (memory \$name size)} declaration.
      Allocation within that chunk must be done by the user, which is a major limitation of WebAssembly when compared to the JavaScript virtual machine.
      WebAssembly is still very much under development and may add support for garbage collection in the future.
      The scope of my project does not include garbage collection, so any memory needed during execution will not be re-used within the heap block.
   \item \textbf{Indirect Calls}
      WebAssembly's support for function tables with the {\tt (table \$name size)} declaration is essential for implementing function closures.
      Without this feature, efficiently compiling functional OCaml would be impossible.
   \item \textbf{Interop with JavaScript}
      WebAssembly has both {\tt import} and {\tt export} labels allowing memory and functions to be shared between Wasm and the enclosing environment.
      This not only gives JavaScript code an entry-point into our compiled code, but it also allows our compiled code to call JavaScript functions and pass between complex results by sharing memory.
      The possibilities allowed by this are huge, letting us sandbox Wasm from the rest of our application while still giving it an interface to our code.
\end{itemize}

\subsection{Why JavaScript}
In my original proposal, I intended to implement the compiler in OCaml for somewhat arbitrary reasons.
I chose OCaml as the implementation language because it matched the source language, I thought its strong yet flexible type system made it a good choice for implementing a compiler, and I wanted to gain experience with the language.
While doing the preparation, the advantages of using JavaScript became clear and I decided to write the compiler in JS.

A big reason to use JavaScript was support for JS by the WebAssembly Binary Toolkit (wabt).
This is the tool by which programs written in the human-readable WebAssembly Text Format ({\tt .wat} files) are validated and converted into runnable programs in the WebAssembly Binary Format ({\tt .wasm} files).
This tool can be used both in browsers and in the Node JS environment, and can be integrated into a JavaScript implementation of the compiler.
An OCaml version outputting {\tt .wat} would require an extra "assembly" step in order to create runnable code.
Additionally, Node and web browsers are currently the only environments supporting WebAssembly, so in order to run code created by the compiler some JavaScript will be necessary.
So, a JavaScript implementation gives us an single encapsulated program for compiling, assembling, and executing OCaml on both the back-end and in the front-end, giving a lot of versatility.

\subsection{Parser Generators}
A parser generator takes input a grammar in some specification language and produces a program which will parse strings in the language of the grammar.
Parser generators are an important tool for compiler designers since it would be impractical and tedious to implement shift-reduce parsers by hand for every language.
These tools are generally used to generate shift-reduce parsers for LR grammars, which can be efficiently parsed and can represent almost all programming language constructs.

Parsers are specified in specialized languages depending on the parser generator, but commonly consist of 3 sections: definitions, rules, and auxiliary functions.
The definitions section describes the tokens used by the grammar, and often supports regular expressions, acting as a lexer before parsing occurs.
The rules section contains the grammar's rules, with each rule having an accompanying piece of code for processing terms reduced using the rule.
The auxiliary function section then contains helper functions that can be called by the code in the rules section.
In the case of a compiler, the code in the parser specification file produce a parse-tree, although other applications like a desk calculator or a simpler translator can be implemented just in the code fed into a parser generator.

The parser generator reads this specification, computes $FIRST$ and $FOLLOW$ sets for the non-terminals, and uses those sets to generate an LALR(1) parsing table.
The code for the reduce action for a row in the table can be filled in with the code for the rule in the specification file.
Not all grammars are LALR(1) though, which will lead to shift-reduce and reduce-reduce conflicts during parser generation.
A convenient feature of parser generators is that they allow you to say how these conflicts are resolved without having to re-write the grammar.
Instead you can add precedence and associativity labels to grammar rules that can resolve grammar ambiguities without adding extra grammar rules.

In order to prepare for this project, I learned to use the tool Jison, which is a JavaScript version of the popular Bison parser generator.
This was a substitution for {\tt ocamllex} and {\tt ocamlyacc} which I had planned to use in an OCaml implementation.
The OCaml language has a well specified grammar (see \url{https://ocaml.org/releases/4.11/htmlman/language.html}) so my task was to translate this grammar in an BNF-like form to Jison.
The main task being reducing this grammar to describe our subset of OCaml and resolving the ambiguity in the provided BNF.
One interesting feature of OCaml's grammar is the use of the juxtaposition operator for function application and type constructors.
This provided an implementation challenge described in the next chapter.

\section{Requirements}
\subsection{OCaml Subset}
Here are the types, operations, and control structures the compiler should support:
\begin{itemize}
   \item
      Primitive types {\tt int}, {\tt bool}, {\tt float}.
   \item
      Operators on primitive types types {\tt +}, {\tt -}, {\tt *}, {\tt /}, {\tt mod}, {\tt +.}, {\tt -.}, {\tt *.}, {\tt /.}, {\tt \&\&}, {\tt ||}.
   \item
      Polymorphic comparison operators {\tt =}, {\tt !=}, {\tt <}, {\tt <=}, {\tt >}, {\tt >=}.
   \item
      Tuple types such as {\tt int * bool} constructed with the comma operator and destructed using {\tt match-with}.
   \item
      Variant types declared with {\tt type} to give us abstract data types. Using this, all other data structures such as lists and trees can be constructed. Polymorphic types will not be supported.
      \begin{verbatim}
type IntTree = Leaf | Branch of IntTree * int * IntTree \end{verbatim}
   \item
      Function types constructed with {\tt fun x -> expr} with function applications.
   \item
      {\tt let} and {\tt let rec} for defining variables and functions (with currying syntax).
   \item
      Branching with {\tt if-then-else}. This is not strictly necessary syntax since we have {\tt match-with}, but it's useful syntactic sugar
   \item
      Pattern matching using {\tt match-with}. This is key because it's the only way to structure tuple and variant types. Full patterns should be supported.

\end{itemize}
\subsection{Sample Programs}
An important part of preparing to start development was writing a set of sample OCaml programs.
These programs should demonstrate and exercises the functionality of the compiler.
Compiling and running these programs to get the correct result is part of the success criteria for the project.
These are essentially end-to-end tests of the system.
So here is a list of the sample programs to be implemented before starting work on the compiler:
\begin{itemize}
   \item Factorial (Recursive)
   \item Fibonacci Numbers (Recursive)
   \item Integer Square Root (Newton's Method)
   \item Grandpa's Age Puzzle (Generate and Test)
   \item Binary Search Tree Insertion / Deletion / Lookup
   \item 8 Queens Problem (Backtracking Search)
   \item Quicksort
\end{itemize}
Most of these are well-known computer science problems with the exception of Grandpa's Age Puzzle.
The puzzle is to find the ages (between 10 and 99 years) of a grandpa and his grandson which meet two conditions: The grandpa's age must be 4 times the grandson's age, and the reverse (18 $\rightarrow$ 81) of the grandson's age must be 3 times the reverse of the grandpa's age.
The ages 18 and 72 are one solution to this puzzle.
This problem was selected because it can be solved using the common generate-and-test pattern.

\subsection{Benchmarking}
Another requirement of this project is benchmarking the compiler in order to evaluate what has been created, and whether it successfully allows programmers to write OCaml for the web as an alternative to JavaScript.

TODO: finish this once I've done the evaluation

\section{Software Engineering}
In this section, we'll talk about the engineering tools and practices used to execute this project.
\subsection{Spiral Model}
\begin{figure}[tbh]
\centerline{\includegraphics{waterfall}}
\caption{Software Development Lifecycle Using the Waterfall Model}
\label{waterfallfig}
\end{figure}
The classic approach to software development is to use a linear "Waterfall" methodology, stepping through all the essential parts of the software development lifecycle.
These key phases include the following: Requirements gathering, Design, Implementation, Testing, and finally Deployment/Maintenance.
Upfront, a timeline for the entire project is specified, allocating maybe 30\% of the project for planning, 40\% for coding the implementation, and the rest for testing/bugfixes and deployment.
Focussing on one complete phase at a time helps to ensure the entire product is cohesive and all the interacting parts have been designed with the whole project in mind.
This type of top-down approach works well when a project is routine, having well-understood requirements that are unlikely to change.

In general, the goal of a software engineering methodology is to minimize risk.
Conventional project management wisdom says that the closer that errors are found to when they were introduced means the least risk to the project's completion.
Conversely, the longer that an error persists, the greater the damage, with errors that make it into the release resulting in a product that either doesn't work or solves the wrong problem and wasn't what the client actually wanted.
A large disadvantage with the waterfall method is that errors in the design won't be caught until months or possibly years later when testing finally starts.
Catching and correcting design errors, even more so than difficult coding bugs can cost months of redesign and reimplementation.
For certain types of projects, it's very common for a client to realize that what they've asked for isn't what they really wanted only after seeing a completed product.
With the waterfall approach, where the client is only able to see the software after it's been fully implemented and tested, it's often too late or too expensive to make requirements changes.

\begin{figure}[tbh]
\centerline{\includegraphics[height=1.75in]{iterative}}
\caption{Software Development Lifecycle Using an Iterative Model}
\label{iterativefig}
\end{figure}
The spiral model was created to minimize risk by catching mistakes in the design and implementation closer to where they were introduced by producing multiple prototypes of the software.
The spiral model is an iterative approach as shown in Figure~\ref{iterativefig}, where we've taken the full waterfall lifecycle and repeated it on a small scale.
The spiral model also entails creating prototype of the full system from each cycle, even if many components are bare-bones or have to be mocked up.
The testing and evaluation of one iteration (or sprint) leads directly into the planning of the next iteration, allowing feedback to be taken on during development instead of after.
This also helps to catch problems with the integration and cohesion of different components that might not be found if end-to-end tests can't be run until the end of the project.
This is key to solving really hard problems where you're doing something that has never been done before, and it's often the case that the faster you fail, the faster you can succeed.
One possible disadvantage of the spiral method is that some effort might be wasted on prototyping components that much be re-written for the full implementation, but ideally the benefits of testing the full system outweigh this cost.

So, how does this all apply to an undergraduate dissertation undertaken by one a 1 person team?
Writing a compiler is a well-studied and understood task, with requirements documented by the project proposal that shouldn't change, which might lend itself to the waterfall method.
At the same time, the interactions between the components of a compiler can be complicated, with changes to the parser output greatly effecting code generator and vice versa.
The proposal is also specifying more like 1.5 projects, with the optional extension as an iterative addition to a finished base project.
The hard submission deadline with harsh penalties for being late also drove me away from a waterfall approach where errors found at the end of the project causing large delays are not acceptable.
So it made sense to me to use the spiral approach or creating a full compiler every few weeks with larger and larger subsets of OCaml until the base project and possibly extensions were finished.
Having a prototype of the compiler would also allow me to get better feedback from my supervisor with full demonstrations of progress.
Some effort was wasted prototyping components that would later be fully rewritten, but these the lessons learned from these prototypes were invaluable in writing the full compiler.

\subsection{Test Driven Development}
Test Driven Development (TDD) describes a broad set of practices with the common element that tests are written before what they're testing has been implemented.
This is often most effective when applied at the scale of unit tests, but can also apply to end-to-end tests as in the case of this project.
So, what are the benefits of writing tests first?
For a 1 person project like mine, writing your own tests can leave large blind spots.
If I've just spent days implementing some functionality, I can hardly test my implementation as if it's a black-box and edge cases missed in the implementation will likely be missed by the tests as well.
Writing tests is also in a sense part of the design process, forcing the programmer to consider the requirements of a component before they start coding.
Another benefit of having tests before you start coding is that you know you've finished once the tests all pass, and any extra work or refactoring is optional.
There's also the obvious benefit that you end up with lots of tests that are not just an afterthought, but can play an active part in the development process.

For these reasons, I started my project by writing the sample OCaml programs described above, adding tests for the expected output, and setting up automated end-to-end testing using the JavaScript test framework Mocha.
Having these sample programs was also invaluable for evaluating the compiler against JavaScript versions of the same programs and against the JS\_of\_OCaml compiled versions.
I started the project by writing smaller unit tests for the parser and code generator, but did not keep up with this formal testing through development.

\subsection{Kanban Organizational Technique}

\begin{figure}[H]
   \centering
   \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[height=1.5in]{kanban-early}
      \caption{The kanban board early in development}
   \end{minipage}\hfill
   \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[height=1.5in]{kanban-late}
      \caption{The kanban board later in development}
   \end{minipage}
\end{figure}
Kanban is a popular method for managing workflows in software engineering.
It can be complicated or simple, but the important feature is you have a visual system for tracking tasks through your workflow.
For my project, this was a wall with sticky notes representing small tasks that needed to be completely for the project taking max a couple of hours.
Breaking the project into these small units is helpful for managing and tracking progress.
My board hard 4 columns: To Do, Today, Blocked, and Done.
At the start of an iterator (or a milestone from the project proposal), the To Do column should be filled with all the tasks necessary to reach finish this iteration's prototype.
Then on a given day spent working on the project, I would select a number of tasks proportional to the time I have to work and move them to the Today column.
If there was something blocking my progress, such as waiting for an answer to a question from my supervisor or another task needing to be completed first, that sticky note would be moved to the Blocked column.
Finally, completed tasks are moved to the done column.
Some of the organizational benefits of using kanban were lost because I'm not working on a team and the scope of the project was probably manageable without this tool, but it still served a purpose for my personal organization and juggling completing several tasks in parallel.

\subsection{Source Control / Backup}
Git was used for both source control with Github as a remote host and backup solution.
Backup was important for this project in the unlikely case that something happens to the files on my local machine, I won't lose all progress.
The project is publicly hosted on github and can be downloaded in the case of disaster.
Git was a necessary tool on this project for tracking changes to the source code and organizing those changes in to meaningful commits.
Commits then become the tangible unit for thinking about progress on the project allowing us to checkout a commit to see a snapshot of the project at a certain point, remove a commit from the history to undo a set of errant changes, or organize the project into branches of related work to be developed in parallel.
The collaborative benefits of Git and Github weren't realized by this work on the project, but could be used in future efforts to improve the compiler.

\chapter{Implementation}

\section{Repository Overview}
\subsection{Pipeline Overview}
\subsection{Filestructure}

\section{Parsing}
For parsing, I've used the powerful Jison port of the parser generator Bison.
This means all the information for parsing is in the {\tt ocaml.jison} file and {\tt parser.js} is just an interface that works with Jison.
I could have used Jison to generate JavaScript source files for a parser, but to allow quick changes to the parser for ease of development, a parser object is generated by Jison each run of the parser.
For a full release though, it would be simple to generate a parser once, and just include that parser in the source of the project.

\subsection{Lexing}
The first stage of the front-end of a compiler is usually the lexer, which groups smaller chunks of characters in the source code into tokens.
Grouping and classifying raw characters in the source can be done using regular expressions, instead of the more powerful context-free grammars for full parsing.
OCaml is white-space insensitive, so the lexer also discards whitespace characters.
This was a simple task as patterns for tokens are described in the OCaml spec. \\
Here are a few example rules for lexing:
\begin{itemize}
   \item {\tt /"-"?[0-9][0-9\_]*/g return 'INT\_LITERAL'}
   \item {\tt /"true"|"false"/g return 'BOOL\_LITERAL'}
   \item {\tt /[a-zA-Z\_][a-zA-Z\_0-9']*/g return 'ident'};
\end{itemize}
The parser will then deal with grammar symbols and tokens such as {\tt 'INT\_LITERAL'} and {\tt 'ident'} as terminals.

\subsection{The Grammar}
The grammar is pretty straightforward with top-level declarations for the important units of a program such as: {\tt expr}, {\tt type-decl}, and {\tt pattern}.
There are then many more grammar symbols for implementing these higher level language features.
Much more important than determining whether a string is in the language is constructing the parse tree.
As we'll see throughout this chapter, our AST is a normal JS object with each node in the tree having a {\tt tokenName} property such as {\tt 'LET'}, {\tt 'INFIX\_OP'}, or {\tt 'FUNC'}.
Along with a tokenName, a node will have properties dependant on the token type, including the objects for child nodes and sub-expressions.
Jison lets us specify code along with a grammar rule, allowing us to construct an object when that rule is reduced, and giving us access to what's been produced by the components of the rule.
As an example, here is the rule for a non-recursive let expression:
\begin{verbatim}
expr:
   'let' let-binding 'in' expr
      {$$ = {tokenName: 'LET', rec: false, binding: $2, body: $4}} \end{verbatim}
\subsection{Resolving Ambiguity}
A direct translation of the grammar in the OCaml specification results in a grammar that's context free, but not in $LR$. We get lots of constructions like the following:
\begin{verbatim} expr:
   expr '+' expr
 | expr '*' expr
 | expr '||' expr \end{verbatim}
This grammar results in many shift-reduce conflicts while constructing the parse table.
My first try at resolving this ambiguity was to annotate the rules to force the parser into making certain reduce decisions when there's a conflict.
We can give Jison an ordered list of annotations such as {\tt \%left '+' '-'; \%left '*' '/' 'mod'}.
The keyword {\tt \%left} (or {\tt \%right}) gives the associativity of the operator and the order gives precedence, so in our example {\tt '*'} binds more tightly than {\tt '+'}.
This method was sufficient in the first few prototypes of the compiler, but I ran into a problem when implementing function applications.

Function application in OCaml is done using the juxtaposition of two expressions.
Unfortunately the rule {\tt expr: expr expr} introduces ambiguities that are not so easy to resolve.
The problem arises from the fact there is no separator between or around the expressions.
While we can give the precedence of this reduction, we cannot give the associativity of a blank operator in Jison.
The rules for expressions (and other constructs using juxtaposition) would have to be rewritten to grammatically fix this ambiguity.
So, the current version of the compiler has the precedence and associativity of operators encoded in the rules themselves.
The new grammar is structured something like this:
\begin{verbatim}
add-expr: mult-expr   | add-expr '+' mult-expr;
mult-expr: unary-expr | mult-expr '*' unary-expr;
unary-expr: app-expr  | '-' unary-expr;
app-expr: val-expr    | app-expr val-expr;
\end{verbatim}
After trying lots of different things with precedence annotations and being stuck on the ambiguity of the juxtaposition operator for a while, switching to this style of grammar worked right away and solved so many problems.

\section{Code Transformations}
A design decision I made was to keep the backend (type inference and code generation) as simple as possible.
There were some redundant language features so useful and convenient that they were essential even in my small subset of OCaml.
Rather than handling these constructs which are almost syntactic sugar throughout the compiler, I chose to translate them to simpler constructions directly after parsing.

\subsection{Match Translation}
Pattern matching is one of the more useful and unique features of OCaml, which has no good analogs in JavaScript.
A {\tt match-with} expression can be decomposed into a set of tests checking whether a pattern has been matched, as well as let declarations for binding identifiers matched in the pattern.
Rather than implementing these checks and declarations at the code generation stage, I chose to translate {\tt match} expressions directly after parsing.

The helper function {\tt unifyPattern(pattern, matcher)} does the key work for translating pattern matching into {\tt IF} and {\tt LET} expressions.
{\tt unifyPattern} returns an object with two lists: a list of boolean expressions checking if the pattern has been matched and a list of let-bindings for variables declared in the pattern.
This function is designed for recursion because to match a tuple-pattern we must recursively match elements of the tuple with sub-expressions in the pattern, accumulating checks and declarations along the way.
Similarly, to unify a type-constructor pattern with a matcher, we need to add a check that the matcher was constructed with that type-constructor and then recursively unify the argument of the pattern with the argument extracted from the matching expression.

With sets of checks and declarations for each pattern, we can translate a {\tt match} expression as follows:
\begin{verbatim}
match e with
   pattern1 -> body1
 | pattern2 -> body2
\end{verbatim}
is translated to:
\begin{verbatim}
if check-1-for-pattern1 and check-2-for-pattern1 and...
   then let decl-1-of-pattern1 in
        let decl-2-of-pattern1 in ...
      in body1
   if check-1-for-pattern2 and check-2-for-pattern2 and...
      then let decl-1-of-pattern2 in
           let decl-2-of-pattern2 in ...
         in body2
      else throw match error
\end{verbatim}

\subsection{Let Translation}
Instead of having multiple ways to declare variables, I chose to carry only variables defined in functions forward into the back-end of the compiler.
This translation is based on the observation that {\tt let x = 2 + 2 in x * x} can be rewritten as {\tt (fun x -> x * x) (2 + 2)}
Let also gives us some extra syntax for defining functions and functions with multiple parameters.
But this can also be translated into function application with {\tt let f x y = x + y in f 2 3} being translated (slightly tediously) into {\tt (f => f 2 3) (fun f -> fun x -> fun y -> x + y)}.
So translating let expressions into function applications, is quite straightforward and only requires some re-arranging and explicit currying for function declarations.

These two translations for match and let allow us to extend the syntax of the language without having to add additional cases to the type inference algorithm or the code generator, which would be a significant amount of work.
Instead, we can piggy-back off already implemented code for function application and {\tt if-then-else}.
\section{Type Checking/Inference}
\section{Code Generation}
\section{The Rest... TODO better title}
\section{A Worked Example}

\chapter{Evaluation}

TODO

\chapter{Conclusion}

TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{Project Proposal}

% \input{proposal}

\end{document}
